{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intelligent Tic Tac Toe Agent using Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "LENGTH = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Intelligent Agent Class "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, eps=0.1, alpha=0.5):\n",
    "        self.eps = eps\n",
    "        self.alpha = alpha\n",
    "        self.verbose = False\n",
    "        self.state_history = []\n",
    "    \n",
    "    def set_V(self, V):\n",
    "        self.V = V\n",
    "    \n",
    "    def set_symbol(self, sym):\n",
    "        self.sym = sym\n",
    "    \n",
    "    def set_verbose(self, verbose):\n",
    "        self.verbose = verbose\n",
    "    \n",
    "    def reset_history(self):\n",
    "        self.state_history = []\n",
    "    \n",
    "    def take_action(self, env):\n",
    "        r = np.random.randn()\n",
    "        best_state = None\n",
    "        \n",
    "        if r < self.eps:\n",
    "            if self.verbose:\n",
    "                print('Taking a random action')\n",
    "            possible_moves = []\n",
    "            for i in range(LENGTH):\n",
    "                for j in range(LENGTH):\n",
    "                    if env.is_empty(i, j):\n",
    "                        possible_moves.append((i, j))\n",
    "            idx = np.random.choice(len(possible_moves))\n",
    "            next_move = possible_moves[idx]\n",
    "        else:\n",
    "            pos2value = {}\n",
    "            next_move = None\n",
    "            best_value = -1\n",
    "            for i in range(LENGTH):\n",
    "                for j in range(LENGTH):\n",
    "                    if env.is_empty(i, j):\n",
    "                        env.board[i, j] = self.sym\n",
    "                        state = env.get_state()\n",
    "                        env.board[i, j] = 0\n",
    "                        pos2value[(i, j)] = self.V[state]\n",
    "                        if self.V[state] > best_value:\n",
    "                            best_value = self.V[state]\n",
    "                            best_state = state\n",
    "                            next_move = (i, j)\n",
    "                            \n",
    "            if self.verbose:\n",
    "                print('Taking a greedy action')\n",
    "                for i in range(LENGTH):\n",
    "                    print('------------------')\n",
    "                    for j in range(LENGTH):\n",
    "                        if env.is_empty(i, j):\n",
    "                            print(' %.2f|' % pos2value[(i,j)], end='')\n",
    "                        else:\n",
    "                            print('  ', end='')\n",
    "                            if env.board[i,j] == env.x:\n",
    "                                print('x  |', end='')\n",
    "                            elif env.board[i,j] == env.o:\n",
    "                                print('o  |', end='')\n",
    "                            else:\n",
    "                                print('   |', end='')\n",
    "                    print('')\n",
    "                print('------------------')\n",
    "        \n",
    "        env.board[next_move[0], next_move[1]] = self.sym\n",
    "    \n",
    "    def update_state_history(self, state):\n",
    "        self.state_history.append(state)\n",
    "    \n",
    "    def update(self, env):\n",
    "        reward = env.reward(self.sym)\n",
    "        target = reward\n",
    "        for prev in reversed(self.state_history):\n",
    "            value = self.V[prev] + self.alpha * (target - self.V[prev])\n",
    "            self.V[prev] = value\n",
    "            target = value\n",
    "        self.reset_history()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Environment Class in which Agent trains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment():\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((LENGTH, LENGTH))\n",
    "        self.x = -1\n",
    "        self.o = 1\n",
    "        self.winner = None\n",
    "        self.ended = False\n",
    "        self.num_states = 3 ** (LENGTH * LENGTH)\n",
    "    \n",
    "    def is_empty(self, i, j):\n",
    "        return self.board[i, j] == 0\n",
    "    \n",
    "    def reward(self, sym):\n",
    "        if not self.game_over():\n",
    "            return 0\n",
    "        return 1 if self.winner == sym else 0\n",
    "    \n",
    "    def get_state(self):\n",
    "        k = 0\n",
    "        h = 0\n",
    "        for i in range(LENGTH):\n",
    "            for j in range(LENGTH):\n",
    "                if self.board[i, j] == 0:\n",
    "                    v = 0\n",
    "                elif self.board[i, j] == self.x:\n",
    "                    v = 1\n",
    "                elif self.board[i, j] == self.o:\n",
    "                    v = 2\n",
    "                h += (3 ** k) * v\n",
    "                k += 1\n",
    "        return h\n",
    "    \n",
    "    def game_over(self, force_recalculate=False):\n",
    "        if not force_recalculate and self.ended:\n",
    "            return self.ended\n",
    "        \n",
    "        for i in range(LENGTH):\n",
    "            for player in (self.x, self.o):\n",
    "                if self.board[i].sum() == player * LENGTH:\n",
    "                    self.winner = player\n",
    "                    self.ended = True\n",
    "                    return True\n",
    "                \n",
    "        for j in range(LENGTH):\n",
    "            for player in (self.x, self.o):\n",
    "                if self.board[:, j].sum() == player * LENGTH:\n",
    "                    self.winner = player\n",
    "                    self.ended = True\n",
    "                    return True\n",
    "                \n",
    "        for player in (self.x, self.o):\n",
    "            if self.board.trace() == player * LENGTH:\n",
    "                self.winner = player\n",
    "                self.ended = True\n",
    "                return True\n",
    "            \n",
    "            if np.fliplr(self.board).trace() == player * LENGTH:\n",
    "                self.winner = player\n",
    "                self.ended = True\n",
    "                return True\n",
    "        \n",
    "        if np.all((self.board == 0) == False):\n",
    "            self.winner = None\n",
    "            self.ended = True\n",
    "            return True\n",
    "        \n",
    "        self.winner = None\n",
    "        return False\n",
    "    \n",
    "    def draw_board(self):\n",
    "        for i in range(LENGTH):\n",
    "            print('-------------')\n",
    "            for j in range(LENGTH):\n",
    "                print('  ', end='')\n",
    "                if self.board[i,j] == self.x:\n",
    "                    print('x ', end='')\n",
    "                elif self.board[i,j] == self.o:\n",
    "                    print('o ', end='')\n",
    "                else:\n",
    "                    print('  ', end='')\n",
    "            print('')\n",
    "        print('-------------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Class for a human player against the trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Human():\n",
    "    \n",
    "    def __init__(self):\n",
    "        pass\n",
    "    \n",
    "    def set_symbol(self, sym):\n",
    "        self.sym = sym\n",
    "    \n",
    "    def take_action(self, env):\n",
    "        while True:\n",
    "            move = input('Enter coordinates i,j for your next move (i,j=0..2): ')\n",
    "            i, j = move.split(',')\n",
    "            i = int(i)\n",
    "            j = int(j)\n",
    "            if env.is_empty(i, j):\n",
    "                env.board[i, j] = self.sym\n",
    "                break\n",
    "    \n",
    "    def update(self, env):\n",
    "        pass\n",
    "    \n",
    "    def update_state_history(self, state):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Initialize state values as follows:\n",
    "- if x wins, V(s) = 1\n",
    "- if x loses or draw, V(s) = 0\n",
    "- otherwise, V(s) = 0.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialV_x(env, state_winner_triples):\n",
    "    V = np.zeros(env.num_states)\n",
    "    for state, winner, ended in state_winner_triples:\n",
    "        if ended:\n",
    "            if winner == env.x:\n",
    "                v = 1\n",
    "            else:\n",
    "                v = 0\n",
    "        else:\n",
    "            v = 0.5\n",
    "        V[state] = v\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- This is (almost) the opposite of initial V for player x\n",
    "- Since everywhere where x wins (1), o loses (0)\n",
    "- But a draw is still 0 for o"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialV_o(env, state_winner_triples):\n",
    "    V = np.zeros(env.num_states)\n",
    "    for state, winner, ended in state_winner_triples:\n",
    "        if ended:\n",
    "            if winner == env.o:\n",
    "                v = 1\n",
    "            else:\n",
    "                v = 0\n",
    "        else:\n",
    "            v = 0.5\n",
    "        V[state] = v\n",
    "    return V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Recursive function that will return all possible states (as ints) and who the corresponding winner is for those states (if any)\n",
    "- (i, j) refers to the next cell on the board to permute (we need to try -1, 0, 1)\n",
    "- Impossible games are ignored, i.e. 3x's and 3o's in a row simultaneously since that will never happen in a real game"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_state_hash_and_winner(env, i=0, j=0):\n",
    "    results = []\n",
    "\n",
    "    for v in (0, env.x, env.o):\n",
    "        env.board[i,j] = v\n",
    "        if j == 2:\n",
    "            if i == 2:\n",
    "                state = env.get_state()\n",
    "                ended = env.game_over(force_recalculate=True)\n",
    "                winner = env.winner\n",
    "                results.append((state, winner, ended))\n",
    "            else:\n",
    "                results += get_state_hash_and_winner(env, i + 1, 0)\n",
    "        else:\n",
    "            results += get_state_hash_and_winner(env, i, j + 1)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Game starting function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def play_game(p1, p2, env, draw=False):\n",
    "    current_player = None\n",
    "    \n",
    "    while not env.game_over():\n",
    "        if current_player == p1:\n",
    "            current_player = p2\n",
    "        else:\n",
    "            current_player = p1\n",
    "        \n",
    "        if draw:\n",
    "            if draw == 1 and current_player == p1:\n",
    "                env.draw_board()\n",
    "            if draw == 2 and current_player == p2:\n",
    "                env.draw_board()\n",
    "        \n",
    "        current_player.take_action(env)\n",
    "        \n",
    "        state = env.get_state()\n",
    "        p1.update_state_history(state)\n",
    "        p2.update_state_history(state)\n",
    "        \n",
    "    if draw:\n",
    "        env.draw_board()\n",
    "    \n",
    "    p1.update(env)\n",
    "    p2.update(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1 = Agent()\n",
    "p2 = Agent()\n",
    "env = Environment()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Remove comments to see the agents getting trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "state_winner_triples = get_state_hash_and_winner(env)\n",
    "Vx = initialV_x(env, state_winner_triples)\n",
    "p1.set_V(Vx)\n",
    "Vo = initialV_o(env, state_winner_triples)\n",
    "p2.set_V(Vo)\n",
    "\n",
    "p1.set_symbol(env.x)\n",
    "p2.set_symbol(env.o)\n",
    "\n",
    "# p1.set_verbose(True)\n",
    "# p2.set_verbose(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "200\n",
      "400\n",
      "600\n",
      "800\n",
      "1000\n",
      "1200\n",
      "1400\n",
      "1600\n",
      "1800\n",
      "2000\n",
      "2200\n",
      "2400\n",
      "2600\n",
      "2800\n",
      "3000\n",
      "3200\n",
      "3400\n",
      "3600\n",
      "3800\n",
      "4000\n",
      "4200\n",
      "4400\n",
      "4600\n",
      "4800\n",
      "5000\n",
      "5200\n",
      "5400\n",
      "5600\n",
      "5800\n",
      "6000\n",
      "6200\n",
      "6400\n",
      "6600\n",
      "6800\n",
      "7000\n",
      "7200\n",
      "7400\n",
      "7600\n",
      "7800\n",
      "8000\n",
      "8200\n",
      "8400\n",
      "8600\n",
      "8800\n",
      "9000\n",
      "9200\n",
      "9400\n",
      "9600\n",
      "9800\n"
     ]
    }
   ],
   "source": [
    "T = 10000\n",
    "for t in range(T):\n",
    "    if t % 200 == 0:\n",
    "        print(t)\n",
    "    play_game(p1, p2, Environment())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Agents are trained\n",
    "- Play with any one of them"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "human = Human()\n",
    "human.set_symbol(env.o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Taking a random action\n",
      "-------------\n",
      "          x \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 2,2\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.69| 0.73|  x  |\n",
      "------------------\n",
      " 0.70| 0.66| 0.43|\n",
      "------------------\n",
      " 0.56| 0.60|  o  |\n",
      "------------------\n",
      "-------------\n",
      "      x   x \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "          o \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 0,0\n",
      "Taking a random action\n",
      "-------------\n",
      "  o   x   x \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "          o \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 2,1\n",
      "Taking a greedy action\n",
      "------------------\n",
      "  o  |  x  |  x  |\n",
      "------------------\n",
      " 0.04|  x  | 0.04|\n",
      "------------------\n",
      " 1.00|  o  |  o  |\n",
      "------------------\n",
      "-------------\n",
      "  o   x   x \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "  x   o   o \n",
      "-------------\n",
      "Play again? [Y/n]: 1,0\n",
      "Taking a greedy action\n",
      "------------------\n",
      " 0.59| 0.47| 0.68|\n",
      "------------------\n",
      " 0.49| 0.72| 0.57|\n",
      "------------------\n",
      " 0.52| 0.60| 0.49|\n",
      "------------------\n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 0,0\n",
      "Taking a greedy action\n",
      "------------------\n",
      "  o  | 0.85| 0.72|\n",
      "------------------\n",
      " 0.60|  x  | 0.63|\n",
      "------------------\n",
      " 0.42| 0.38| 0.59|\n",
      "------------------\n",
      "-------------\n",
      "  o   x     \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "            \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 2,1\n",
      "Taking a greedy action\n",
      "------------------\n",
      "  o  |  x  | 0.30|\n",
      "------------------\n",
      " 0.57|  x  | 0.27|\n",
      "------------------\n",
      " 0.96|  o  | 0.31|\n",
      "------------------\n",
      "-------------\n",
      "  o   x     \n",
      "-------------\n",
      "      x     \n",
      "-------------\n",
      "  x   o     \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 0,2\n",
      "Taking a random action\n",
      "-------------\n",
      "  o   x   o \n",
      "-------------\n",
      "  x   x     \n",
      "-------------\n",
      "  x   o     \n",
      "-------------\n",
      "Enter coordinates i,j for your next move (i,j=0..2): 1,2\n",
      "Taking a random action\n",
      "-------------\n",
      "  o   x   o \n",
      "-------------\n",
      "  x   x   o \n",
      "-------------\n",
      "  x   o   x \n",
      "-------------\n",
      "Play again? [Y/n]: n\n"
     ]
    }
   ],
   "source": [
    "while(True):\n",
    "    p1.set_verbose(True)\n",
    "    play_game(p1, human, Environment(), draw=2)\n",
    "    answer = input('Play again? [Y/n]: ')\n",
    "    if answer and answer.lower()[0] == 'n':\n",
    "        break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
